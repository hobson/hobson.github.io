Acronym to help me remember what to talk about this morning. My profound thought last night involved something about: 

It expanded over time as I faded into sleep... AAD then AADR, and this morning added a few more related words. Could be rearranged to be RADAR with any subset of these.

AADDDRR...EE  RADAR DEED ?
Analogy -- As in that large book about how all intelligence is about analogy
Abstraction -- How deep learning networks are trying to abstract data into features
Dialog -- Chatbots exchanging ideas with each other and humans with representations that have "meaning" for both parties
Depth -- Deep learning and deep thoughts involve layers of abstraction and complex interactions between those abstractions
Dream -- Dreaming is the spontaneous, random, uncontrolled, unfocused emission of thought in order to reset/clean the brain and assimilate thought into cohesive echo states or oscillators that can be pulled from the next day.
Responsive (Response) -- Chatbots are generally nonresponsive to questions, generating next words in a sequence that quickly diverge from the seed question or statement.
Representation -- Abstractions and representations are where the power of a machine learning model or Deep Network lies. Dending on the encoding, embedding, representation a network can be deep or shallow, effective or useless, slow to learn or immediately useful.
Encoding
Embedding


The meaning in life, thought, mind, can only be seen in dialog, between "lobes", the representation of the world that emerges from a mind in a way that is designed to be understood by another mind. It uses a common frame of rreference, and shared common knowledge to create these representations. And they've evolved over billions of years to be a useful and compact representation.

Current deep learning language models focus on predicting the next word to emerge in sequence of words. They require bootstrapping or "burn-in" as Hinton calls it. "Echo Networks" are created with a random representation of the world that is shufled and recombined to produce an output. Sequences can be triggered by any event and can even be auto-generated to represent internal will, impetus.

However what emerges makes sense to to nothing but he machine. It gives insight into its knowledge about the statistical relationship between words and concepts, but not in a way that is useful. It's not answering any question or responding to any perceived need in the world.

Machines must learn to dialog, answer questions directly, not be seeded with open ended sentences, like "The meaning of life is..." or "Once upon a time..."  These are so open-ended and the distribution of hidden to output weights is so uniform that the random selection of the next character determines the process and it essentially evolves from a random "thought." To create intelligence it must compose a sentence with a purpose, have an idea of what it wants to say in whole not in part. That concept can evolve and shift during the saying of it. But allowing it to say short things and feed off its dialog partner will allow a larger intelligence and understanding to emerge than what a single mind or brain could manage.

My vague basket of words to represent this thought and convey it to myself to seed the writing of this passage is what we need to do with machines. When we ask them a question or make a statement to them, they should generate a thought, a concept, a representation of a concept in the form of a few word vectors. These can then be used as the guard-rails for the generation of a sentence that must say something logical and interesting and responsive to the statement by the other mind/person in the dialog.

Loved the discussion of character based models and the meaning of "sn" morpheme (not recognized as a morpheme officially, but it has meaning). Words that start with sn deal with the nose and upper lip, snort, sniff, snot, snarf, snarl, snuggle, snear. But what about snow (Hinton mentioned snorting cocaine/snow). But what about snake (it has no nose)?